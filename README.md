# ğŸš€ Real-Time Data Pipeline with Kafka, Flink & ClickHouse

> A comprehensive streaming analytics platform built from the ground up over 12 weeks

[![Python Version](https://img.shields.io/badge/python-3.12%2B-blue.svg)](https://www.python.org/downloads/)
[![Apache Flink](https://img.shields.io/badge/Apache%20Flink-1.18-red.svg)](https://flink.apache.org/)
[![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-3.6-black.svg)](https://kafka.apache.org/)
[![ClickHouse](https://img.shields.io/badge/ClickHouse-23.8-yellow.svg)](https://clickhouse.com/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## ğŸ“– Overview

This project demonstrates a production-grade real-time data analytics pipeline that:

- **Captures** events from various sources into Apache Kafka
- **Processes** streams in real-time using Apache Flink
- **Stores** data in ClickHouse (OLAP) and PostgreSQL (transactional)
- **Visualizes** insights through dashboards and REST APIs
- **Monitors** system health with Prometheus & Grafana

**Architecture Overview:**
```
Data Sources â†’ PostgreSQL â†’ Kafka â†’ Flink (Stream Processing) â†’ ClickHouse (Analytics)
                                            â†“                          â†“
                                      Monitoring Stack            REST API + Dashboard
```

---

## âœ¨ Features

- [ ] **Event-Driven Architecture**: Real-time data ingestion with Kafka
- [ ] **Stream Processing**: Complex event processing with Apache Flink
- [ ] **OLAP Analytics**: High-performance queries with ClickHouse
- [ ] **Metadata Storage**: Relational data management with PostgreSQL
- [ ] **Monitoring**: Full observability with Prometheus & Grafana
- [ ] **Data Quality**: Validation, error handling, and reconciliation
- [ ] **API Layer**: REST endpoints for data access
- [ ] **Visualization**: Real-time dashboards
- [ ] **Containerized**: Complete Docker Compose setup

---

## ğŸ—ï¸ Architecture

### System Components

| Component | Purpose | Technology |
|-----------|---------|------------|
| **Message Broker** | Event streaming platform | Apache Kafka + Zookeeper |
| **Stream Processor** | Real-time data transformation | Apache Flink (PyFlink) |
| **OLAP Database** | Analytical queries | ClickHouse |
| **OLTP Database** | Transactional data | PostgreSQL |
| **Monitoring** | Metrics & alerts | Prometheus + Grafana |
| **API Layer** | Data access | FastAPI |
| **Visualization** | Dashboards | Streamlit / Grafana |

### Data Flow

```
1. Data Generation â†’ PostgreSQL seed data + synthetic event generators
2. Event Production â†’ Python producers publish to Kafka topics
3. Stream Processing â†’ Flink jobs consume, transform, and aggregate
4. Data Storage â†’ Processed data written to ClickHouse
5. Analytics â†’ REST API queries ClickHouse for insights
6. Monitoring â†’ All components emit metrics to Prometheus
```

---

## ğŸ“ Project Structure

```
flink_poc/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ producers/                # Kafka producers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â”‚   â”œâ”€â”€ postgres_reader.py
â”‚   â”‚   â””â”€â”€ generators/           # Synthetic data generators
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ ecommerce_generator.py
â”‚   â”‚       â”œâ”€â”€ iot_generator.py
â”‚   â”‚       â””â”€â”€ saas_generator.py
â”‚   â”œâ”€â”€ consumers/                # Kafka consumers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ kafka_consumer.py
â”‚   â”œâ”€â”€ flink_jobs/               # Flink streaming jobs
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ basic_streaming.py
â”‚   â”‚   â”œâ”€â”€ transformations/      # Stream transformations
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ windowed_aggregations.py
â”‚   â”‚   â”‚   â”œâ”€â”€ enrichment.py
â”‚   â”‚   â”‚   â””â”€â”€ stateful_processing.py
â”‚   â”‚   â””â”€â”€ aggregations/         # Time-based aggregations
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ minute_aggregations.py
â”‚   â”‚       â”œâ”€â”€ hourly_aggregations.py
â”‚   â”‚       â””â”€â”€ daily_aggregations.py
â”‚   â”œâ”€â”€ api/                      # REST API
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ analytics.py
â”‚   â”‚   â”‚   â””â”€â”€ health.py
â”‚   â”‚   â””â”€â”€ dependencies.py
â”‚   â”œâ”€â”€ dashboard/                # Visualization layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ charts.py
â”‚   â”œâ”€â”€ models/                   # Data models & schemas
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ events.py
â”‚   â”‚   â”œâ”€â”€ schemas.py
â”‚   â”‚   â””â”€â”€ database.py
â”‚   â”œâ”€â”€ utils/                    # Utility functions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ kafka_utils.py
â”‚   â”‚   â”œâ”€â”€ db_utils.py
â”‚   â”‚   â”œâ”€â”€ validation.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ config/                   # Configuration
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ settings.py
â”‚       â””â”€â”€ logging_config.py
â”œâ”€â”€ docker/                       # Docker configurations
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ flink/
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ clickhouse/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ init-db.sql
â”‚   â”œâ”€â”€ postgres/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ init-db.sql
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ prometheus/
â”‚       â”‚   â””â”€â”€ prometheus.yml
â”‚       â””â”€â”€ grafana/
â”‚           â””â”€â”€ dashboards/
â”œâ”€â”€ configs/                      # YAML configs
â”‚   â”œâ”€â”€ kafka.yaml
â”‚   â”œâ”€â”€ flink.yaml
â”‚   â”œâ”€â”€ clickhouse.yaml
â”‚   â””â”€â”€ postgres.yaml
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ setup_environment.sh
â”‚   â”œâ”€â”€ start_services.sh
â”‚   â”œâ”€â”€ stop_services.sh
â”‚   â”œâ”€â”€ seed_database.py
â”‚   â””â”€â”€ run_benchmarks.py
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_producers.py
â”‚   â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â”‚   â””â”€â”€ test_utils.py
â”‚   â””â”€â”€ integration/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_pipeline.py
â”‚       â””â”€â”€ test_api.py
â”œâ”€â”€ data/                         # Data files
â”‚   â”œâ”€â”€ seed/
â”‚   â”‚   â”œâ”€â”€ users.json
â”‚   â”‚   â”œâ”€â”€ products.json
â”‚   â”‚   â””â”€â”€ categories.json
â”‚   â””â”€â”€ schemas/
â”‚       â”œâ”€â”€ events_schema.json
â”‚       â””â”€â”€ kafka_topics.json
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ architecture/
â”‚   â”‚   â”œâ”€â”€ system_design.md
â”‚   â”‚   â””â”€â”€ data_flow.md
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ endpoints.md
â”‚   â””â”€â”€ setup/
â”‚       â”œâ”€â”€ local_development.md
â”‚       â””â”€â”€ production_deployment.md
â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”‚   â”œâ”€â”€ exploratory_analysis.ipynb
â”‚   â””â”€â”€ performance_testing.ipynb
â”œâ”€â”€ monitoring/                   # Monitoring configs
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â”œâ”€â”€ system_health.json
â”‚   â”‚       â””â”€â”€ pipeline_metrics.json
â”‚   â””â”€â”€ prometheus/
â”‚       â””â”€â”€ alerts.yml
â”œâ”€â”€ .env.example                  # Environment variables template
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ pyproject.toml                # Python dependencies (uv)
â”œâ”€â”€ uv.lock                       # Lock file
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ LICENSE                       # MIT License
â”œâ”€â”€ PROJECT_ROADMAP.md            # 12-week development plan
â””â”€â”€ WEEKLY_PROGRESS.md            # Weekly progress tracker
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.12+**
- **Docker & Docker Compose**
- **uv** (Python package manager)
- **Git**

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/flink_poc.git
cd flink_poc

# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install Python dependencies
uv sync

# Copy environment variables
cp .env.example .env
# Edit .env with your configuration

# Start all services with Docker Compose
cd docker
docker-compose up -d

# Verify services are running
docker-compose ps

# Run database initialization scripts
cd ../scripts
./setup_environment.sh
```

### Quick Start

```bash
# 1. Start the infrastructure
docker-compose -f docker/docker-compose.yml up -d

# 2. Seed the database with initial data
uv run python scripts/seed_database.py

# 3. Start a Kafka producer
uv run python src/producers/kafka_producer.py

# 4. Submit a Flink job
uv run python src/flink_jobs/basic_streaming.py

# 5. Access the services
# - Kafka UI: http://localhost:8080
# - Flink Dashboard: http://localhost:8081
# - Grafana: http://localhost:3000
# - API: http://localhost:8000/docs
```

---

## ğŸ“š Documentation

- **[Project Roadmap](docs/PROJECT_ROADMAP.md)** - 12-week development plan with learning resources
- **[Weekly Progress](docs/WEEKLY_PROGRESS.md)** - Track your 3-hour weekly sessions
- **[Architecture Documentation](docs/architecture/)** - System design and data flow
- **[API Documentation](docs/api/)** - REST API endpoints and usage
- **[Setup Guides](docs/setup/)** - Local development and deployment

---

## ğŸ¯ Use Cases

This project template can be adapted for various domains:

### ğŸ›’ E-Commerce Analytics
- Track user behavior (page views, searches, purchases)
- Analyze conversion funnels and revenue metrics
- Monitor product performance and inventory

### ğŸŒ¡ï¸ IoT Sensor Platform
- Process sensor readings in real-time
- Detect anomalies and generate alerts
- Predict maintenance needs

### ğŸ’¼ SaaS Application Monitoring
- Track feature usage and user engagement
- Monitor errors and performance metrics
- Analyze user journey and retention

### ğŸ’° Financial Transaction Processing
- Process transactions in real-time
- Detect fraudulent patterns
- Generate regulatory compliance reports

---

## ğŸ”§ Development

### Running Tests

```bash
# Run all tests
uv run pytest

# Run unit tests only
uv run pytest tests/unit/

# Run integration tests
uv run pytest tests/integration/

# Run with coverage
uv run pytest --cov=src tests/
```

### Code Quality

```bash
# Format code
uv run ruff format src/

# Lint code
uv run ruff check src/

# Type checking
uv run mypy src/
```

### Monitoring

Access the monitoring dashboards:

- **Grafana**: http://localhost:3000 (admin/admin)
- **Prometheus**: http://localhost:9090
- **Flink Dashboard**: http://localhost:8081

---

## ğŸ“Š Performance

### Benchmarks

| Metric | Target | Current |
|--------|--------|---------|
| Events/second throughput | 10,000+ | TBD |
| End-to-end latency (p99) | < 5s | TBD |
| Query response time (p95) | < 500ms | TBD |
| Data accuracy | 99.9%+ | TBD |

*Benchmarks will be updated as the project progresses*

---

## ğŸ—“ï¸ Development Timeline

This project follows a 12-week learning path with 3-hour weekly sessions:

- **Weeks 1-2**: Foundation (Kafka, PostgreSQL)
- **Weeks 3-4**: Stream Processing (Flink basics & transformations)
- **Weeks 5-6**: OLAP Integration (ClickHouse & pipelines)
- **Weeks 7-8**: Data Generation & Advanced Analytics
- **Weeks 9-10**: Monitoring & Data Quality
- **Weeks 11-12**: Optimization & API Layer

See [PROJECT_ROADMAP.md](docs/PROJECT_ROADMAP.md) for detailed weekly plans.

---

## ğŸ¤ Contributing

This is a personal learning project, but suggestions and improvements are welcome!

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

### Learning Resources

- **Apache Kafka**: [Official Documentation](https://kafka.apache.org/documentation/)
- **Apache Flink**: [PyFlink Documentation](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/python/overview/)
- **ClickHouse**: [Official Docs](https://clickhouse.com/docs/en/intro)
- **Stream Processing**: [Designing Data-Intensive Applications](https://dataintensive.net/) by Martin Kleppmann

### Communities

- [Confluent Community (Kafka)](https://www.confluent.io/community/)
- [Apache Flink Community](https://flink.apache.org/community.html)
- [ClickHouse Community](https://clickhouse.com/community)

---

## ğŸ“§ Contact

**Project Maintainer**: [Your Name]

- GitHub: [@yourusername](https://github.com/yourusername)
- Email: your.email@example.com
- LinkedIn: [Your Profile](https://linkedin.com/in/yourprofile)

---

## ğŸ“ Learning Journey

Track your progress in [WEEKLY_PROGRESS.md](docs/WEEKLY_PROGRESS.md)

**Current Week**: Week 1  
**Total Hours**: 0/36  
**Completion**: 0%

---

## ğŸ”® Future Enhancements

- [ ] Add schema registry (Confluent Schema Registry)
- [ ] Implement exactly-once semantics
- [ ] Add machine learning model serving
- [ ] Implement CDC (Change Data Capture) from PostgreSQL
- [ ] Add Redis caching layer
- [ ] Kubernetes deployment manifests
- [ ] CI/CD pipeline with GitHub Actions
- [ ] Multi-region support
- [ ] A/B testing framework
- [ ] Real-time recommendation engine

---

<div align="center">
  
**Built with â¤ï¸ as a learning journey into real-time data engineering**

â­ Star this repo if you find it helpful!

</div>

